{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/quora-insincere-questions-classification/test.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/train.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/sample_submission.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings/paragram_300_sl999/README.txt\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings/paragram_300_sl999/paragram_300_sl999.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import time\n",
    "pd.set_option('max_colwidth',400)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"F-score is ill-defined and being set to 0.0 due to no predicted samples.\")\n",
    "import re\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/quora-insincere-questions-classification/train.csv\")\n",
    "test = pd.read_csv(\"../input/quora-insincere-questions-classification/test.csv\")\n",
    "sub = pd.read_csv('../input/quora-insincere-questions-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available embeddings: ['GoogleNews-vectors-negative300', 'glove.840B.300d', 'wiki-news-300d-1M', 'paragram_300_sl999']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('Available embeddings:', os.listdir(\"../input/quora-insincere-questions-classification/embeddings/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1225312\n",
       "1      80810\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province as a nation in the 1960s?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you encourage people to adopt and not shop?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity affect space geometry?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg hemispheres?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain bike by just changing the tyres?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  \\\n",
       "0  00002165364db923c7e6   \n",
       "1  000032939017120e6e44   \n",
       "2  0000412ca6e4628ce2cf   \n",
       "3  000042bf85aa498cd78e   \n",
       "4  0000455dfa3e01eae3af   \n",
       "\n",
       "                                                                       question_text  \\\n",
       "0           How did Quebec nationalists see their province as a nation in the 1960s?   \n",
       "1  Do you have an adopted dog, how would you encourage people to adopt and not shop?   \n",
       "2                Why does velocity affect time? Does velocity affect space geometry?   \n",
       "3                          How did Otto von Guericke used the Magdeburg hemispheres?   \n",
       "4      Can I convert montra helicon D to a mountain bike by just changing the tyres?   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of questions in train is 13.\n",
      "Average word length of questions in test is 13.\n"
     ]
    }
   ],
   "source": [
    "print('Average word length of questions in train is {0:.0f}.'.format(np.mean(train['question_text'].apply(lambda x: len(x.split())))))\n",
    "print('Average word length of questions in test is {0:.0f}.'.format(np.mean(test['question_text'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max word length of questions in train is 134.\n",
      "Max word length of questions in test is 87.\n"
     ]
    }
   ],
   "source": [
    "print('Max word length of questions in train is {0:.0f}.'.format(np.max(train['question_text'].apply(lambda x: len(x.split())))))\n",
    "print('Max word length of questions in test is {0:.0f}.'.format(np.max(test['question_text'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of questions in train is 71.\n",
      "Average character length of questions in test is 71.\n"
     ]
    }
   ],
   "source": [
    "print('Average character length of questions in train is {0:.0f}.'.format(np.mean(train['question_text'].apply(lambda x: len(x)))))\n",
    "print('Average character length of questions in test is {0:.0f}.'.format(np.mean(test['question_text'].apply(lambda x: len(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^{|}~' + '“”’':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '_`':\n",
    "        x = x.replace(punct, ' ')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:24<00:00, 54168.71it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'quora' : 'question and answer website',\n",
    "                'Quoran':'user' , 'Quorans' : 'user' ,\n",
    "                'BITSAT':'exam' , \n",
    "                'COMEDK':'exam' , \n",
    "                'NMAT':'exam',\n",
    "                'KVPY':'exam' , \n",
    "                'WBJEE':'exam' , \n",
    "                'VITEEE':'exam' , \n",
    "                'UCEED':'exam' , \n",
    "                'UPSEE':'exam', \n",
    "                'AMCAT':'exam',\n",
    "                'IITJEE':'exam',\n",
    "                'mtech' : 'master of engineering',\n",
    "                'articleship' : 'internship',\n",
    "                'UPES' : 'University of Petroleum and Energy Studies',\n",
    "                'aadhar' : 'identification number',\n",
    "                'adhar':'identification number',\n",
    "                'marksheet' : 'record tool',\n",
    "                'Fortnite' : 'online game',\n",
    "                'AFCAT' : 'Air Force Common Admission Test',\n",
    "                'bcom' : 'Bachelor of Commerce',\n",
    "                'dropshipping' : 'drop shipping',\n",
    "                'BNBR' : 'be nice be respectful',\n",
    "                'IITian' : 'alumni',\n",
    "                'ICOs' : 'inital coin offerings',\n",
    "                'L&T':'company',\n",
    "                'JIIT':'university',\n",
    "                'LNMIIT':'university',\n",
    "                'Zerodha':'company',\n",
    "                'Kavalireddi':'person',\n",
    "                'Binance':'company',\n",
    "                'R&D':'research and develop',\n",
    "                'etc…':'etc',\n",
    "                'Doklam':'area',\n",
    "                'AT&T':'company',\n",
    "                'NICMAR':'National Institute of Construction Management and Research',\n",
    "                'Vajiram':'institue',\n",
    "                'fiitjee':'exam',\n",
    "                'Unacademy':'company',\n",
    "                'D&D':'game',\n",
    "                'MUOET':'test',\n",
    "                'WooCommerce':'plugin',\n",
    "                'INFJs':'acronym',\n",
    "                'chsl':'exam',\n",
    "                'Modiji':'person',\n",
    "                'HackerRank':'web',\n",
    "                'AlShamsi':'company',\n",
    "                'Q&A':'question and answer',\n",
    "                'Bhakts':'a group of people',\n",
    "                'bhakts':'a group of people',\n",
    "                'Awdhesh':'person',\n",
    "                'eLitmus':'company',\n",
    "                'J&K':'region',\n",
    "                'AIQ':'Artificial Intelligence and Intelligence Quotient',\n",
    "                'PUBG':'game',\n",
    "                '&amp':'HTML',\n",
    "                'CHSL':'exam',\n",
    "                'coinbase':'secure platform',\n",
    "                'SRMJEE':'exam',\n",
    "                'rahu':'astronomical bodies',\n",
    "                'h1b':'visa',\n",
    "                'Skripal':'person',\n",
    "                'SGSITS':'college',\n",
    "                'S&P':'stork market index',\n",
    "                'jipmer':'exam',\n",
    "                'bahubali':'person',\n",
    "                'Zebpay':'digital wallet',\n",
    "                'MeToo':'movement',\n",
    "                'BMSCE':'college',\n",
    "                'PDPU':'university',\n",
    "                'Whatare':'what are',\n",
    "                'Howdo':'how do',\n",
    "                'josaa':'Government agency',\n",
    "                'SRMJEEE':'exam',\n",
    "                'Golang':'program language',\n",
    "                'upwork':'platform',\n",
    "                'BIPC':'course',\n",
    "                'M&A':'Mergers and acquisitions',\n",
    "                'MHCET':'exam',\n",
    "                'mastrubation':'masturbation',\n",
    "                'JBIMS':'university',\n",
    "                'arihant':'ship',\n",
    "                'CDSE':'exam',\n",
    "                'tanx':'tan x',\n",
    "                'playstore':'app'\n",
    "                }\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:19<00:00, 68109.34it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 120000\n",
    "tk = Tokenizer(lower = True, filters='', num_words=max_features)\n",
    "full_text = list(train['question_text'].values) + list(test['question_text'].values)\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tk.texts_to_sequences(train['question_text'].fillna('missing'))\n",
    "test_tokenized = tk.texts_to_sequences(test['question_text'].fillna('missing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['question_text'].apply(lambda x: len(x.split())).plot(kind='hist');\n",
    "plt.yscale('log');\n",
    "plt.title('Distribution of question text length in characters');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 72\n",
    "maxlen = 72\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "splits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=10).split(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_size = 300\n",
    "# embedding_path = \"../input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
    "# def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "# embedding_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_path, encoding='utf-8', errors='ignore'))\n",
    "# all_embs = np.stack(embedding_index.values())\n",
    "# emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "# emb_mean,emb_std = -0.005838499, 0.48782197\n",
    "# word_index = tk.word_index\n",
    "# nb_words = min(max_features, len(word_index))\n",
    "# embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words + 1, embed_size))\n",
    "# for word, i in word_index.items():\n",
    "#     if i >= max_features: continue\n",
    "#     embedding_vector = embedding_index.get(word)\n",
    "#     if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "embed_size = 300\n",
    "embedding_path = \"../input/quora-insincere-questions-classification/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\"\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_path, encoding='utf-8', errors='ignore') if len(o)>100)\n",
    "all_embs = np.stack(embedding_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "# emb_mean,emb_std = -0.0053247833, 0.49346462\n",
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        hidden_size = 128\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)# 120000, 300\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))#[120001, 300]\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(512, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x) #[512, 72, 300]\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))#[512, 72, 300]\n",
    "        h_lstm, _ = self.lstm(h_embedding)#[512, 72, 256]\n",
    "        avg_pool = torch.mean(h_lstm, 1)#[512, 256]\n",
    "        max_pool, _ = torch.max(h_lstm, 1)#[512, 256]\n",
    "        conc = torch.cat((avg_pool, max_pool), 1)#[512, 512]\n",
    "        conc = self.relu(self.linear(conc))#[512, 16]\n",
    "        conc = self.dropout(conc)#[512, 16]\n",
    "        out = self.out(conc)#[512, 1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_val, y_val, validate=True):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # scheduler = CosineAnnealingLR(optimizer, T_max=5)\n",
    "    # scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    valid = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean').cuda()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "            y_pred = model(x_batch)\n",
    "            \n",
    "            \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        model.eval()\n",
    "        \n",
    "        valid_preds = np.zeros((x_val_fold.size(0)))\n",
    "        \n",
    "        if validate:\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                y_pred = model(x_batch).detach()\n",
    "\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                valid_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            search_result = threshold_search(y_val.cpu().numpy(), valid_preds)\n",
    "\n",
    "            val_f1, val_threshold = search_result['f1'], search_result['threshold']\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} best_t={:.2f} \\t time={:.2f}s'.format(\n",
    "                epoch + 1, n_epochs, avg_loss, avg_val_loss, val_f1, val_threshold, elapsed_time))\n",
    "        else:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
    "                epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
    "    \n",
    "    valid_preds = np.zeros((x_val_fold.size(0)))\n",
    "    \n",
    "    avg_val_loss = 0.\n",
    "    for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "\n",
    "        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        valid_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    print('Validation loss: ', avg_val_loss)\n",
    "\n",
    "    test_preds = np.zeros((len(test_loader.dataset)))\n",
    "    \n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "\n",
    "        test_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "    # scheduler.step()\n",
    "    \n",
    "    return valid_preds, test_preds#, test_preds_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cuda = torch.tensor(X_test, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "batch_size = 512\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1029\n",
    "\n",
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)], disable=True):\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/7 \t loss=0.1483 \t time=54.80s\n",
      "Epoch 2/7 \t loss=0.1235 \t time=53.63s\n",
      "Epoch 3/7 \t loss=0.1175 \t time=54.95s\n",
      "Epoch 4/7 \t loss=0.1134 \t time=54.65s\n",
      "Epoch 5/7 \t loss=0.1095 \t time=53.78s\n",
      "Epoch 6/7 \t loss=0.1046 \t time=54.83s\n",
      "Epoch 7/7 \t loss=0.1004 \t time=54.65s\n",
      "Validation loss:  0.10172019024207302\n",
      "Fold 2\n",
      "Epoch 1/7 \t loss=0.1561 \t time=53.67s\n",
      "Epoch 2/7 \t loss=0.1323 \t time=54.43s\n",
      "Epoch 3/7 \t loss=0.1270 \t time=54.39s\n",
      "Epoch 4/7 \t loss=0.1229 \t time=53.60s\n",
      "Epoch 5/7 \t loss=0.1195 \t time=54.53s\n",
      "Epoch 6/7 \t loss=0.1166 \t time=54.58s\n",
      "Epoch 7/7 \t loss=0.1136 \t time=54.32s\n",
      "Validation loss:  0.10220630375265513\n",
      "Fold 3\n",
      "Epoch 1/7 \t loss=0.1395 \t time=53.86s\n",
      "Epoch 2/7 \t loss=0.1206 \t time=54.14s\n",
      "Epoch 3/7 \t loss=0.1147 \t time=54.99s\n",
      "Epoch 4/7 \t loss=0.1102 \t time=54.86s\n",
      "Epoch 5/7 \t loss=0.1054 \t time=54.28s\n",
      "Epoch 6/7 \t loss=0.1007 \t time=54.87s\n",
      "Epoch 7/7 \t loss=0.0955 \t time=54.91s\n",
      "Validation loss:  0.1020786274212842\n",
      "Fold 4\n",
      "Epoch 1/7 \t loss=0.1558 \t time=54.78s\n",
      "Epoch 2/7 \t loss=0.1238 \t time=54.05s\n",
      "Epoch 3/7 \t loss=0.1180 \t time=55.21s\n",
      "Epoch 4/7 \t loss=0.1139 \t time=53.85s\n",
      "Epoch 5/7 \t loss=0.1101 \t time=54.89s\n",
      "Epoch 6/7 \t loss=0.1054 \t time=54.84s\n",
      "Epoch 7/7 \t loss=0.1005 \t time=53.90s\n",
      "Validation loss:  0.10059748449856207\n"
     ]
    }
   ],
   "source": [
    "train_preds = np.zeros(len(train))\n",
    "test_preds = np.zeros((len(test), len(splits)))\n",
    "n_epochs = 7\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):    \n",
    "    x_train_fold = torch.tensor(X_train[train_idx], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(y_train[train_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    x_val_fold = torch.tensor(X_train[valid_idx], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(y_train[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f'Fold {i + 1}')\n",
    "    \n",
    "    seed_everything(seed + i)\n",
    "    model = NeuralNet()\n",
    "    model.cuda()\n",
    "\n",
    "    valid_preds_fold, test_preds_fold = train_model(model,x_train_fold,y_train_fold,x_val_fold,y_val_fold, validate=False)\n",
    "\n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds[:, i] = test_preds_fold\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = threshold_search(y_train, train_preds)\n",
    "sub['prediction'] = test_preds.mean(1) > search_result['threshold']\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
